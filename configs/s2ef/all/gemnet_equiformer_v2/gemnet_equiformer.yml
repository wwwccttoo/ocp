trainer: force

dataset:
  train:
    src: data/s2ef/all_md/train/
    normalize_labels: True
    target_mean: -0.7554450631141663
    target_std: 2.887317180633545
    grad_target_mean: 0.0
    grad_target_std: 2.887317180633545
  val:
    src: data/s2ef/all/val_id/


logger: wandb

task:
  dataset: lmdb
  type: regression
  train_on_position_vectors: True
  train_on_free_atoms: True
  strict_load: False

model:
  name: gemnet_equiformer_v2
  num_spherical: 7
  num_radial: 128
  num_blocks: 3
  after_freeze_numblocks: 1
  emb_size_atom: 512
  emb_size_edge: 512
  emb_size_trip: 64
  emb_size_rbf: 16
  emb_size_cbf: 16
  emb_size_bil_trip: 64
  num_before_skip: 1
  num_after_skip: 2
  num_concat: 1
  num_atom: 3
  cutoff: 6.0
  max_neighbors: 50
  rbf:
    name: gaussian
  envelope:
    name: polynomial
    exponent: 5
  cbf:
    name: spherical_harmonics
  extensive: True
  otf_graph: True
  output_init: HeOrthogonal
  activation: silu
  scale_file: configs/s2ef/all/gemnet/scaling_factors/gemnet-dT.json

  regress_forces: True
  direct_forces: True
  freeze: True
  after_freeze_numblocks: 0
  attn_type: multi
  num_heads: 5
  add_positional_embedding: False

  # equiformer part
  use_pbc:                  True
  regress_forces:           True
  otf_graph:                True
  max_neighbors_gh:         20
  max_radius:               12.0
  max_num_elements:         90

  num_layers:               20
  sphere_channels:          128
  attn_hidden_channels:     64              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96.
  num_heads_gh:             8
  attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True.
  attn_value_channels:      16
  ffn_hidden_channels:      128
  norm_type:                'layer_norm_sh' # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

  lmax_list:                [6]
  mmax_list:                [3]
  grid_resolution:          18              # [18, 16, 14, None] For `None`, simply comment this line.

  num_sphere_samples:       128

  edge_channels:            128
  use_atom_edge_embedding:  True
  distance_function:        'gaussian'
  num_distance_basis:       512         # not used

  attn_activation:          'silu'
  use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention.
  ffn_activation:           'silu'      # ['silu', 'swiglu']
  use_gate_act:             False       # [False, True] Switch between gate activation and S2 activation
  use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.

  alpha_drop:               0.1         # [0.0, 0.1]
  drop_path_rate:           0.1         # [0.0, 0.05]
  proj_drop:                0.0

  weight_init:              'uniform'    # ['uniform', 'normal']

optim:
  batch_size: 1
  eval_batch_size: 4
  num_workers: 2
  lr_initial: 1.e-5
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  scheduler: ReduceLROnPlateau
  mode: min
  factor: 0.8
  patience: 3
  max_epochs: 10000
  force_coefficient: 100
  energy_coefficient: 1
  ema_decay: 0.999
  clip_grad_norm: 10
  loss_energy: mae
  loss_force: l2mae
